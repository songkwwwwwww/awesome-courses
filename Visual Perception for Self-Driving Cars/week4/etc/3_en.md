[MUSIC] Last lesson we learned a baseline approach to perform object detection using convenance. However, processing all the anchors in our anchor grid led to multiple bounding boxes being detected per object rather than the expected single locks per object. This lesson, we will discuss the final components we need to build and train convolutional neural networks for 2D object detection. Specifically, you will learn how to handle multiple regressed anchors per object during training through mini batch selection and during inference through non-maximum suppression. Let's begin by reviewing neural network training. We are given our cond net model and training data pairs, x, the input image, and f star of x, the bounding box locations and class. We want to approximate f star of x with our output bounding boxes y equal to f of x and theta. Recall from last week that we performed training by first evaluating a loss function that measures how similar our predicted bounding boxes are to the ground truth bounding boxes. Then we feed the resultant loss function to our optimizer that outputs a new set of parameters theta to be used for the second iteration. Notice that both the feature extractor and the output layers are modified during training. Now if f star of x and f of x theta are one to one, our problem would have been easy. However, the outputs of our network is multiple boxes that can be associated with a single ground truth box. Let's see how we can work to resolve this issue. Remember that for each pixel in the feature map, we associate k anchors. Where do these anchors appear in the original image? As we've learnt earlier, our feature extractor reduces the resolution of the initial input by a factor of 32. That means that if we associate every pixel in the feature map with a set of anchors, these anchors will be transferred to the initial image by placing them on a grid with stride 32. We can then visualize the ground truth bounding box alongside these anchors. You can notice that some anchors overlap and some don't. We quantify this overlap with IOU and categorize the anchors into two categories. We first specify two IOU thresholds, a positive anchor threshold, and a negative anchor threshold. Any anchor with an IOU greater than the positive anchor threshold is called a positive anchor. And similarly, any anchor with an IOU less than the negative anchor threshold is called a negative anchor. Any anchor with an IOU in between the two thresholds is fully discarded. So now, how do we use these positive and negative anchors in training? Let's now see how to assign the classification and regression targets for the positive and negative anchors. For classification, we want the neural network to predict that the negative anchors belong to the background class. Background is usually a class we add to our classes of interest to describe anything non-included in these classes. On the other hand, we want the neural network to assign ground truth class to any positive anchor intersecting that ground truth. For regression, we want to shift the parameters of the positive anchor to be aligned with those of the ground truth bounding box. The negative anchors are not used in bounding box regression as they are assumed to be background. This approach of handling multiple regressed anchors during training is not free from problems. The proposed IOU thresholding mechanism results in most of the regressed anchors being negative anchors. When training with all these anchors, the network will be observing far more negative than positive examples leading to a biased towards the negative class. The solution to this problem is actually quite simple, instead of using all anchors to compute the lost function, we sample the chosen minibatch size with a three to one ratio of negative to positive anchors. The negatives are chosen through a process called online hard negative mining, in which negative minibatch members are chosen as the negative anchors with the highest classification loss. This means that where we've training to fix the biggest errors in negative classification. As an example, if we have a minibatch of 64 examples, the negative minibatch will be the 48 negative examples with the highest classification loss, and the 16 remaining anchors will be positive anchors. If the number of positives is less than 16, we either copy some of the positives to pad the minibatch or fill the remaining spots with negative anchors. As we described earlier last week, we used the cross entropy loss function for the classification head of our ConvNet. The total classification loss is the average of the cross entropy loss of all anchors in the minibatch. The normalization constant and total is the chosen minibatch size. Si is the output of the classification head. And Si star is the ground truth classification which is set to background for negative anchors and to the class of the ground truth bounding box for the positive anchors. For regression, we use the L2 norm loss in a similar manner. However, we only attempt to modify an anchor if it is a positive anchor. This is expressed mathematically with a multiplier Pi on the L2 norm. It is 0 if the anchor is negative and 1 if the anchor is positive. To normalize, we divide by the number of positive anchors, and just as a reminder, bi star is the ground truth bounding box representation, while bi is the estimated bounding box. Remember that we don't directly estimate box parameters, but rather, we modify the anchor parameters by an additive residual or a multiplicative scale. So bi must be constructed from the estimated residuals. Let's visualize what we are trying to teach the neural network to learn with these loss functions. Given an input image, a ground truth bounding box, and a set of input anchors from the anchor prior, we are teaching the neural network to classify anchors as containing background in purple or a car in blue. This is done by minimizing the cross entropy loss defined above. Then we want the neural network to move only anchors that contain a class of interest, in a way that matches the closest ground truth bounding box. This is done by minimizing the L2 norm loss defined above. By now, you should have a good grasp on how to handle multiple output boxes for object during training. But what do we do when we run the neural network in real time during inference? Remember, during inference, we do not have ground truths to determine positive and negative anchors and we do not evaluate loss functions. We just want a single output box per object in the scene. Here is when non max suppression comes into play, an extremely powerful approach to improving inference output for anchor based neuron networks. Non-max suppression takes as an input a list of predicted boundary boxed b, and each bounding blocks is comprised of the regressed coordinates in the class output score. It also needs as an input a predefined IOU threshold which we'll call ada. The algorithm then goes as follows, we first sort the bounding boxes in list B according to their output score. We also initialize an empty set D to hold output bounding boxes. We then proceed to iterate overall elements in the sorted box list B bar. Inside the for loop, we first determine the box B max with the highest score in the list B, which should be the first element in B bar. We then remove this bounding box from the bounding box set D bar and add it to the output set D. Next, we find all boxes remaining in the set B bar that have an IOU greater than ada with the box B max. These boxes significantly overlap with the current maximum box, B max. Any box that satisfies this condition gets removed from the list B bar. We keep iterating through the list B bar until is empty, and then we return the list D. D now contains a single bounding box per object. Let's go through a visual example to understand how non-max suppression algorithms work in practice. Let's assume that we have sorted the bounding box list in decreasing order. We also show the score list explicitly here for better visibility on how non-max suppression works. b max will be the first bounding box of the sorted list B bar. We then proceed to compare each bounding box to b max. In this case, only one box, B3, has a non zero IOU with b max. We compute that IOU and compare it with our IOU threshold ada. In this case, the IOU is greater than the threshold ada, so we remove box 3 from the list B bar. We repeat the process for the next highest score that remains in the list. Again, only one box has a non-zero IOU with b max, box 4 in this case. Computing the IOU and comparing with the threshold, we eliminate box 4 from list B bar, and add box 2 to the output list D. We notice that our initial list, B bar, is now empty. So our non-max suppression algorithm exits and returns the output box list D that contains one bounding box per object as expected. Congratulations, you have now completed the content required to train and deploy ConvNet based 2D object detectors for self-driving cars. In this video, we explored how to adjust network output during training to maintain class balance, and to restrict network output during inference to select one output bounding box per object. In the next lesson, we will discuss how we can use the output of these 2D object detectors for a variety of tasks that are important for self driving cars. Including transforming 2D object detection to 3D, tracking object motion, and applying 2D object detection to traffic sign, and signal detection. See you there. [MUSIC]