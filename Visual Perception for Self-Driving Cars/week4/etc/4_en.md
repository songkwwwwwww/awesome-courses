Hello, and welcome to the last lesson of the Object Detection Module. So far, we have described how to perform 2D object detection. However, we still have to discuss why 2D object detection is useful for self-driving cars. This lesson, we'll discuss three important applications that require 2D object detection to be performed. First, we will discuss how we can extend our 2D object detectors to 3D. Second, we'll discuss object tracking and important tool for modeling the behavior of other agents on the road. Finally, we'll discuss how to apply concepts from 2D object detection to detecting traffic signs and signals. Self-driving cars require scene understanding in 3D to be able to safely traverse their environment. Knowing where pedestrians, cars, lanes, and signs are around the car entirely defines what actions can be taken safely when autonomous. This means that detecting objects in the image plane is not enough. We need to extend the problem from 2D to 3D, and locate the detected objects in the world frame. 3D object detection with ConvNets is a relatively new topic, and results in this domain are constantly changing. As with it's 2D counterpart, 3D object detection requires category classification. This is essential for tracking objects as cars move differently from pedestrians, for example, so better predictions are possible if the object class is known. In addition, we want to estimate the position of the objects centroid in 3D, the extent in 3D, and the orientation in 3D. In each case, this detailed state information improves motion prediction, and collision avoidance, and improves the cars ability to move in traffic. This object's state can be expressed as a 3D vector for centroid position expressed as the x, y, y position of the object. A 3D vector for extense, expressed as the length, width, and height of an object. A 3D vector of orientation angles expressed as the roll, pitch, and yaw angles of an object. For road scenes, the orientation angle we are interested in is usually only the yaw angle as most road agents cannot vary their roll and pitch angles beyond what the ground surface dictates for the most part. It is therefore sufficient to only track the yaw angle. But how do we get from a 2D bounding box to an accurate 3D estimation of the location and extent of an object? The most common and successful way to extend 2D object detection results in 3D is to use LiDAR point clouds. Given a 2D bounding box in an image space and a 3D LiDAR point cloud, we can use the inverse of the camera projection matrix to project the corners of the bounding box as rays into the 3D space. The polygon intersection of these lines is called a frustum, and usually contains points in 3D that correspond to the object in our image. We then take the data in this frustum from the LiDAR, transform it to a representation of our choice, and train a small neural network to predict the seven parameters required to define our bounding box in 3D. Now, which representation of the LiDAR points in the frustum should we input to the neural network? Some groups choose to directly process the raw point cloud data. Others choose to normalize the point cloud data with respect to some fixed point such as the center of the frustum. Finally, one could also preprocess the points to build fixed length representations such as a histogram of x, y, z had points, for example, making their use as an input to a continent much more convenient. Whatever representation we use, we are expected to get results in the form of oriented 3D bounding boxes. Keep in mind that the procedure discussed above is only one way of performing 3D object detection. So, why would we choose to extend 2D detections to 3D rather than detecting objects directly in 3D? First, 2D object detectors are much more well-established than 3D object detectors. We are usually able to get a very high precision and recall from a mature 2D object detector. Something that is still not available when working in the 3D object detection literature. Second, we get classification for free from the 2D object detector results. There is no need to use LiDAR data or pass 3D information into the network to determine whether we are looking at a car or a post. This is eminently visible in the image data in 2D. Finally, searching a 3D space for possible objects is quite computationally expensive if no assumptions can be made about where the object should be found. Extending 2D object detectors to 3D, usually allows us to limit the search region for object instances keeping real-time performance manageable. However, extending 2D object detectors to 3D also incurs a set of unique problems. One prominent problems stemming from using this approach for 3D object detection is that we bound the performance of the 3D pose estimator by an upper limit which is the performance of the 2D detector. Furthermore, 2D to 3D methods usually fail when facing severe occlusion and truncation from the cameras viewpoint which may not affect the LiDAR data. Finally, the latency induced by the serial nature of this approach is usually not negligible. Latency is manifested as delayed perception which means that our car will see an object on the road after a certain delay. If this delay is significant, the system might not be safe enough to operate as vehicle reaction time is limited by perception latency. Another very important application of 2D to 3D object detection is object tracking. Tracking involves stitching together a sequence of detections of the same object into a track that defines the object motion over time. We will begin by describing a simple tracking method that can be used both in 2D and in 3D which will hopefully sound familiar from course two. When we perform object detection, we usually detect objects independently in each frame. In tracking however, we incorporate a predicted position usually through known object dynamic models. Tracking requires a set of assumptions constraining how quickly a scene changes. For example, we assume that our camera and the tracked objects cannot teleport to different locations within a very short time. Also, we assume that the scene changes smoothly and gradually. All of these assumptions are logically valid in roads scenes. Let's visually see what object tracking looks like. Given a detected object in the first frame along with their velocity vectors, we begin by predicting where the objects will end up in the second frame. If we model their motion using the velocity vector, we then get new detections in the second frame. We call these detections or measurements, we correlate each detection with a corresponding measurement, and then update our object prediction using the correlated measurement. Let's describe each of the necessary steps. First, we define a block state as its position and velocity in image space. Each object will have a motion model that updates its state. As an example, the constant velocity motion model shown here is used to move each bounding box to a new location in the second frame. Notice that, we added a zero mean Gaussian noise to our motion model as the model is not perfect. After the prediction step, we get the second frame measurement from our 2D object detector. We then proceed to correlate each measurement to a prediction by computing the IOU between all predictions and all measurements. A measurement is correlated to a corresponding prediction if it has the highest IOU with that prediction. The final step consists of using a Kalman filter to fuse the measurement and prediction updates. We recommend reviewing the Kalman filter equations presented in course two to remember how this step is performed. The Kalman filter updates the whole object state including both the position and the velocity, and we can use that in our subsequent prediction step. Note that, we do not need to track the object sizes but can rely on the detector instead. We usually assign an object the size of its last known measurement. A few intricacies remain to be tackled, specifically how to initiate tracks and how to terminate them. We initiate new tracks if we get 2D detector results not correlated to any previous prediction. Similarly, we terminate inconsistent tracks, if a predicted object does not correlate with a measurement for a preset number of frames. Finally, we need to note that by defining IOU in 3D, we can use the same methodology for 3D object tracking. Let's take a look at a video from the autonomous vehicle 3D object tracking in action. Note that, the tracks are slightly jittery because the detections have some noise in their frame to frame position estimates, and the motion model used in this code does not include accurate vehicle kinematic or dynamic models. This is because of a lack of knowledge of the other vehicles steering and acceleration inputs. Nonetheless, we can still accurately detect multiple vehicles moving through the environment and make careful decisions about precedence and safe interaction along the way. One final concept we will describe within the framework of 2D object detection, is detection of traffic signs and traffic signals. The correct detection of these road rule indicators guides the self-driving car as it drives legally on busy streets. However, the detection task incurs its own separate set of challenges. Let's take a look at the most prominent ones. Here you can see a typical dash cam style image from a camera mounted on a self-driving car. Usually, traffic signs and traffic signals have to be detected at long range for a car to know how to react properly in a timely manner. At long range, traffic signs and signals occupy a very small number of pixels in the image making the detection problem particularly challenging. Furthermore, traffic signs are highly variable. Usually, including as many as 50 classes that need to be classified reliably. Traffic lights on the other hand might appear differently in different areas of the world, and have multiple states that need to be detected for a self-driving car to safely maneuver through signalized intersections. In addition, traffic lights change state as the car moves. This might cause some problems when trying to track traffic signals in image space. As the appearance changes with respect to the state the traffic light is in. Luckily, standard object detectors we have described so far can be used without major modifications to detect traffic signs and signals. However, current approaches rely on multistage hierarchical models to perform this detection task more robustly. Let's consider the two-stage model shown here. The two stages share the output of the feature extractor to perform their respective task. In this example, the first stage outputs class agnostic bounding boxes that point to all traffic signs and signals in the image, without specifying which class each box belongs to. The second stage then takes all of the bounding boxes from the first stage and classifies them into categories such as red, yellow or green signals stop signs etc. In addition, some methods also use the second stage to further refine the bounding boxes provided in the first stage. This multi-stage approach is not specific to traffic signs and signals, and many of the general object detection frameworks employ multi-stage methods to generate accurate object class and location. If interested, we have provided some of these approaches as supplementary reading materials. In this video, we saw that the output of the 2D object detectors can be used to produce 3D object locations, we studied how tracking can be performed on a 2D object detector output in consecutive image frames to create object tracks through the environment, and we explored how 2D object detectors can be used to detect traffic signs and traffic signals without major modifications. However, specialized methods that exploit hierarchical models usually perform better than standard methods. We now have the tools needed for the main types of object detection used in self-driving cars. Congratulations. You've successfully completed this module on 2D object detection. This module has been quite involved and we recommend that you check our provided resources for more information, on how to use neural networks for 2D and 3D object detection and tracking. Next week, we'll discuss another important component of the visual perception stack which operates at the pixel level, semantic segmentation.  See you then.