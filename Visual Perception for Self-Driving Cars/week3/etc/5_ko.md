[음악] 마지막 수업에서, 우리는 데이터 세트를 훈련, 검증 및 테스트 분할로 나누고 이러한 각 분할에서 손실 함수를 평가한 결과를 해석하는 방법을 설명했습니다. 우리는 또한 대부분의 경우 네트워크를 훈련한 후 과복하기보다는 과복으로 고통받는 경향이 있다고 강조한다. 이 수업에서는 교육 중에 정규화 전략을 적용하여 과적합을 줄이는 몇 가지 방법을 살펴볼 것입니다. 정규화의 결과로, 우리의 네트워크는 새로운 데이터로 잘 일반화될 것이며, 실험실 밖에서 더 효과적으로 사용할 수 있을 것이다. 장난감 예시에서 신경망 개발의 반복을 살펴봅시다. 우리는 2D 데카르트 공간을 주황색과 파란색의 두 가지 구성 요소로 분리하고 싶습니다. 파란색 공간에 속하는 모든 점은 클래스 1로 표시되어야 하며, 주황색 공간에 속하는 모든 점은 클래스 2로 표시되어야 합니다. 그러나, 우리는 이러한 클래스나 그들의 경계에 직접 접근할 수 없다. 대신 우리는 점과 해당 클래스의 예를 제공하는 센서 측정에만 접근할 수 있습니다. 불행하게도, 우리의 센서도 시끄럽습니다, 그것은 때때로 잘못된 라벨을 제공한다는 것을 의미합니다. 라벨은 파란색 공간을 클래스 2로, 주황색 공간은 클래스 1로 가리킨다. 우리의 문제는 시끄러운 센서 데이터에서 공간 분류를 찾는 것과 같다. 우리는 센서에서 데이터를 수집하여 60-40개의 훈련 검증 분할로 나누는 것으로 시작합니다. 훈련 분할은 화이트 아웃 라인이 있는 점으로 여기에 표시되며, 검증 분할은 블랙아웃 라인이 있는 포인트로 여기에 표시됩니다. 한 층과 두 개의 숨겨진 단위가 있는 간단한 뉴런 네트워크를 사용하여 측정을 분류해 봅시다. 이 디자인 선택을 사용하여, 우리는 다음과 같은 공간 분류를 얻습니다. 훈련 세트 손실은 0.2268의 유효성 검사 세트 손실에 가깝다. 하지만 그것은 여전히 0.1의 최소 달성 가능한 손실보다 훨씬 높다. 이것은 적절한 경우이다. 네트워크 분류 결과를 실제 공간 분류와 비교할 때, 신경망이 당면한 문제의 복잡성을 포착하지 못하고 필요에 따라 공간을 네 개의 구획으로 올바르게 분할하지 못했다는 것을 알 수 있습니다. 적절한 문제를 해결하기 위해, 우리는 5개의 추가 레이어를 추가하여 네트워크 크기를 늘리고, 숨겨진 유닛 수를 레이어당 6개로 늘립니다. 우리의 모델은 이제 훨씬 더 표현력이 풍부하므로, 진정한 분류를 더 잘 표현할 수 있어야 한다. 우리는 계속해서 모델을 다시 훈련시킨 다음, 우리가 얼마나 잘 해냈는지 테스트합니다. 우리는 0.45의 검증 세트 손실 결과가 0.1의 훈련 세트 손실 결과보다 훨씬 높다는 것을 알아차렸다. 그러나 훈련 세트 손실은 이 작업에서 0.1의 최소 달성 가능한 손실과 같다. 우리는 훈련 데이터에 과장된 상태에 있다. 과복합은 훈련 데이터의 소음을 배우는 네트워크로 인해 발생합니다. 신경망에는 매개 변수가 너무 많기 때문에, 빨간색 원 안에 표시된 것처럼 시끄러운 훈련 예시에 해당하는 공간의 작은 영역을 구부릴 수 있다. 이것은 보통 당면한 문제에 비해 네트워크 크기를 너무 많이 늘릴 때 발생한다. 다시 말하지만, 우리는 피팅을 치료하는 한 가지 방법은 정규화를 통해서라는 것을 배웠다. 신경망에 일반적으로 사용되는 첫 번째 정규화 방법을 확인해 봅시다. 신경망에 적용할 수 있는 가장 전통적인 형태의 정규화는 매개 변수 규범 처벌의 개념이다. 이 접근법은 목적 함수에 세타의 페널티 오메가를 추가하여 모델의 용량을 제한한다. 가중치 매개 변수 알파를 사용하여 기존 손실 함수에 표준 페널티를 추가합니다. 알파는 손실 함수의 총 가치에 대한 표준 페널티의 상대적 기여를 가중하는 새로운 하이퍼파라미터이다. 보통, 세타의 오메가는 세타의 가치가 얼마나 큰지에 대한 척도이다. 가장 일반적으로 이 조치는 Lp 규범이다. P가 1일 때 우리는 절대 합계를 가지고 있고, P가 2일 때 우리는 이차 합계 등을 얻는다. 게다가, 우리는 보통 신경망의 가중치만 제한한다. 이것은 가중치의 수가 신경망의 편향 수보다 훨씬 많다는 사실에 의해 동기 부여된다. 따라서 체중 벌금은 최종 네트워크 성능에 훨씬 더 큰 영향을 미친다. 신경망에서 사용되는 가장 일반적인 규범 처벌은 L2 규범 페널티이다. L2 규범 페널티는 신경망의 각 층의 모든 가중치의 L2 규범을 최소화하려고 한다. L2 규범 페널티가 우리 문제에 적용된 효과를 살펴봅시다. 우리의 최신 디자인은 훈련 데이터 세트에 과장되었다는 것을 기억하세요. L2 규범 페널티를 추가하면 손실 함수는 비정규화된 네트워크에 대한 낮은 검증 세트 손실로 인해 공간 분류를 훨씬 더 잘 추정할 수 있습니다. 그러나, 이 낮은 검증 세트 손실은 훈련 세트 손실이 0.1에서 0.176으로 증가하는 것과 결합된다. 이 경우 일반화 격차의 감소는 훈련 세트 손실의 증가보다 높다. 그러나, 다시 한 번 적절한 정권에 빠지지 않도록 너무 많이 정규화하지 않도록 주의하십시오. 대부분의 신경망 패키지에서 표준 페널티를 추가하는 것은 매우 쉽다. 피팅이 의심된다면, L2 규범 처벌은 설계 과정에서 많은 시간 낭비를 방지하는 매우 쉬운 치료법일 수 있습니다. 이 비디오의 앞부분에서 언급했듯이, 연구자들은 신경망에 특정한 정규화 메커니즘을 개발했다. 정기적으로 사용되는 강력한 메커니즘 중 하나는 탈락이라고 불린다. 네트워크 교육 중에 중퇴가 어떻게 적용되는지 보자. 중퇴자의 첫 번째 단계는 우리가 P 서브 keep라고 부를 확률을 선택하는 것이다. 모든 훈련 반복에서, 이 확률은 네트워크에 보관할 네트워크 노드의 하위 집합을 선택하는 데 사용됩니다. 이 노드들은 숨겨진 단위, 출력 단위 또는 입력 단위일 수 있다. 그런 다음 이 장치에서 나오는 모든 연결을 절단한 후 출력 y를 평가합니다. 우리는 아마도 유지에 비례하는 유닛을 제거하고 있기 때문에, 우리는 훈련이 끝날 때 최종 가중치를 P 서브 유지로 곱합니다. 이것은 전체 네트워크에 대한 추론으로 전환할 때 출력을 잘못 확장하지 않는 데 필수적이다. 드롭아웃은 누락된 입력과 숨겨진 단위로 모델을 강제로 배우도록 강요하는 것으로 직관적으로 설명될 수 있다. 즉, 그 자체의 다른 버전으로. 그것은 훈련 과정에서 광범위한 신경망 모델을 정규화하는 계산적으로 저렴하지만 강력한 방법을 제공하여 수유와 연습을 크게 줄인다. 게다가, 탈락은 사용할 수 있는 훈련 절차의 유형이나 모델을 크게 제한하지 않는다. 그것은 분산된 매개 변수 표현을 사용하는 거의 모든 모델에서 잘 작동하며, 확률적 그라데이션 하강으로 훈련할 수 있다. 마지막으로, 모든 신경망 라이브러리에는 드롭아웃 계층이 구현되어 사용할 준비가 되어 있습니다. 밀집된 피드 포워드 신경망 계층이 있을 때마다 드롭아웃을 사용하는 것이 좋습니다. 당신이 알아야 할 정규화의 최종 형태는 일찍 멈추는 것이다. 조기 정지를 시각적으로 설명하기 위해, 우리는 훈련 세트에서 평가된 신경망의 손실 기능의 진화를 살펴본다. 충분한 용량을 감안할 때, 신경망이 훈련 데이터를 암기하기 때문에 훈련 손실은 0에 가까운 값으로 감소할 수 있어야 한다. 그러나, 독립적인 훈련과 검증 세트가 있다면, 검증 손실은 증가하기 시작하는 지점에 도달한다. 이 행동은 과적합 체제 동안 전형적이며, 조기 중단으로 알려진 방법을 통해 해결될 수 있다. 우리는 이전에 다양한 정지 기준에 따라 최적화를 중단할 수 있다고 논의했다. 사전 설정된 반복 횟수에 대한 검증 손실이 계속 증가할 때 조기 정지는 훈련을 끝낸다. 이것은 보통 신경망이 과적합 체제에 들어가기 직전에 해석된다. 훈련 알고리즘을 중지한 후, 유효성 검사 손실이 가장 낮은 매개 변수 집합이 반환됩니다. 마지막으로, 조기 정지는 정규화를 위한 첫 번째 선택으로 사용해서는 안 된다. 또한 교육 시간을 제한하기 때문에 전반적인 네트워크 성능을 방해할 수 있습니다. 축하합니다, 이제 자신만의 신경망을 구축할 준비가 되었습니다. 이 수업에서, 당신은 과적합 체제에 빠지면서 신경망의 성능을 향상시키는 방법을 배웠습니다. 신경망 설계 및 훈련에는 더 많은 흥미로운 측면이 있으며, 이 모듈에 포함시킨 추가 리소스를 통해 이 매혹적인 분야를 계속 탐구할 것을 촉구합니다. 이 모듈의 다음이자 마지막 수업에서, 우리는 비전 기반 인식에 대한 매우 실용적이고 역사적으로 중요한 신경망 아키텍처인 컨볼루션 신경망에 대해 이야기할 것입니다. 그때 보자 [음악]