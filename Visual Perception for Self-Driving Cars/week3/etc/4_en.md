So far, in this module, we've discussed what a neural network is and how to arrive at the best weights given training data. However, we still need to explore more deeply how to train neural networks efficiently. This lesson will discuss how to split your data for an unbiased estimate of performance on your neural network model and what insights you can get from observing your neural network's performance on various data splits. Let's begin by describing the usual data splits we use to evaluate a machine learning system. Let's take as an example a real life problem. We're given a dataset of 10,000 images of traffic signs with corresponding classification labels. We want to train our neural network to perform traffic sign classification. How do we approach this problem? Do we train on all the data and then deploy our traffic sign classifier? That approach is guaranteed to fail for the following reason. Given a large enough neural network, we are almost guaranteed to get a very low training loss. This is due to the very large number of parameters in a typical deep neural network allowing it to memorize the training data to a large extent given a large enough number of training iterations. A better approach is to split this data into three parts, the training split, the validation split, and the testing split. As the name suggests, the training split is directly observed by the model during neural network training. The loss function is directly minimized on this training set. But as we've stated earlier, we are expecting the value of this function to be very low over the set. The validation split is used by developers to test the performance of the neural network when hyperparameters are changed. Hyperparameters are those parameters that either modify our network structure or affect our training process, but are not a part of the network parameters learned during training. Examples include; the learning rate, the number of layers, the number of units per layer, and the activation function type. The final split is called the testing split and is used to get an unbiased estimate of performance. The test splits should be off limits when developing a neural network architecture so that the neural network never sees this data during the training or hyperparameter optimization process. The only use of the test set should be to evaluate the performance of the final architecture and hyperparameter set before it is deployed on a self-driving vehicle. Let us now determine what percentage of data goes into each split. Before the big data error, it was common to have datasets on the order of thousands of examples. In that case, the default percentage of data that goes into each split was approximately 60 percent for training, 20 percent for validation, and 20 percent held in reserve for testing. However, nowadays, it is not uncommon to have datasets on the order of millions of examples having 20 percent of the data in the validation and test sets is unnecessary as the validation and test would contain far more samples than are needed for the purpose. In such cases, we would find that a training set of 98 percent of the data with a validation and test set of one percent of the data each is not uncommon. Let us go back to our traffic sign detection problem. We assume that our traffic sign dataset is comprised of 10,000 labeled examples. We can separate our dataset into a training validation and testing split according to the 602020 small dataset heuristic. We now evaluate the performance of our neural network on each of these splits using the loss function. For a classification problem, the loss function is defined as the cross entropy between the prediction and the ground truth labels. Cross entropy is strictly greater than zero, so the higher its value, the worse the performance of our classifier. Keep in mind that the neural network only directly observes the training set. All the developers use the validation set to determine the best hyperparameters to use. The ultimate goal of the training is still minimizing the error on the test set since it is an unbiased estimate of performance of our system and the data has never been observed by the network. Let us first consider the following scenario. Let us assume that our estimator gave a cross entropy loss of 0.21 on the training set, and a loss of 0.25 on the validation set, and finally, a loss of 0.3 on the test set. Furthermore, due to errors in the labels of the dataset, the minimum cross entropy loss that we can expect is 0.18. In this case, we have quite a good classifier as the loss on the three sets are fairly consistent and the loss is close to the minimum achievable loss on the entire task. Let's consider a second scenario where the training loss is now 1.9 around ten times that of the minimum loss. As we discussed in the previous lesson, we expect any reasonably sized neural network to be able to almost perfectly fit the data given enough training time. But in this case, the network was not able to do so. We call this scenario where the neural network fails to bring the training loss down underfitting. One other scenario we might face is when we have a low training set loss but a high validation and testing set loss. For example, we might arrive at the case where the validation loss is around ten times that of the training loss. This case is referred to as overfitting and is caused by the neural network optimizing its parameters to precisely reproduce the training data output. When we deploy on the validation set, the network cannot generalize well to the new data. The gap between training and validation loss is called the generalization gap. We want this gap to be as low as possible while still having low enough training loss. Let's see how we can try to go from the underfitting or overfitting regime to a good estimator. We begin with how to remedy underfitting. The first option to remedy underfitting is to train longer. If the architecture is suitable for the task at hand, training longer usually leads to a lower training loss. If the architecture is too small, training longer might not help. In that case, you would want to add more layers to your neural network or add more parameters per layer. If both of the above options don't help, your architecture might not be suitable for the task at hand and you would want to try a different architecture to reduce underfitting. Now, let's proceed to the most common approaches to reduce overfitting. In the case of overfitting, the easiest thing to do is to just collect more data. Unfortunately, for self-driving cars, collecting training data is very expensive as it requires engineering time for data collection and a tremendous amount of annotator time to properly define the true outputs. Another solution for overfitting is regularization. Regularization is any modification made to the learning algorithm with an intention to lower the generalization gap but not the training loss. If all else fails, the final solution is to revisit the architecture and check if it is suitable for the task at hand. In this lesson, we have learned how to interpret the different performance scenarios of our neural network on the training, validation, and test splits. If it is determined that our network is underfitting, the easiest solution is to train for a longer time or to use a larger neural network. However, a much more commonly faced scenario in self-driving car perception is overfitting where good performance on the training data does not always translate to good performance on actual robots. In the next lesson, we'll focus on how to mitigate the effects of overfitting with various regularization strategies. These strategies will allow perception algorithms trained to excel unlabeled datasets to continue to work well when driving through the ever-changing world around us.